ğŸ“° Headline Generation using Encoder-Decoder Architectures
This project focuses on automatic headline generation using various sequence-to-sequence (Seq2Seq) architectures. It compares three different approaches to generate news headlines from article text.

ğŸ“Œ Architectures Implemented
1. ğŸ” LSTM/GRU Encoder-Decoder (Without Attention)
A traditional Seq2Seq model using LSTM/GRU networks with a fixed-length context vector from the encoder to initialize the decoder.

2. ğŸ¯ Encoder-Decoder with Attention (Bahdanau / Luong)
An enhanced model where the decoder can dynamically attend to relevant parts of the input sequence at each step:

Bahdanau Attention (Additive)

Luong Attention (Multiplicative)

3. ğŸ§  Transformer (Self-Attention)
A modern architecture based on self-attention mechanisms that completely replaces RNNs:

Positional Encoding

Multi-head Attention

Layer Normalization and Feedforward layers

ğŸ§ª Dataset
Source: Kaggle - News Headline Generation Dataset

Consists of news articles and corresponding headlines.

Preprocessed by tokenization, lowercasing, and padding.

ğŸ§  Features
âœ… Implemented using TensorFlow 
âœ… Includes:

Embedding layers

Regularization: Dropout, L2 weight decay

Activation functions: ReLU, Softmax

Custom attention layers

Inference mode for evaluation

ğŸ“‰ Evaluation Metrics:

BLEU Score

ROUGE Score

ğŸ“Š Model Comparison
Model	Attention	BLEU / ROUGE (sample)	Notes
LSTM Encoder-Decoder	âŒ None	--	Struggles with long sentences
LSTM with Bahdanau/Luong	âœ… Yes	--	More accurate and context-aware
Transformer (Self-Attn)	âœ… Full Self-Attn	--	Fastest and most scalable

(Add your actual scores once available)

ğŸš€ How to Run
Clone the repo and install dependencies:

bash
Copy
Edit
git clone https://github.com/your-username/headline-generation.git
cd headline-generation
pip install -r requirements.txt
Download the dataset from Kaggle and place it in a data/ folder.

Run the notebook or training scripts for each model:

bash
Copy
Edit
python train_lstm.py
python train_attention.py
python train_transformer.py
Evaluate and compare outputs using BLEU/ROUGE scores.

ğŸ“ Project Structure
kotlin
Copy
Edit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ news_dataset.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lstm_encoder_decoder.py
â”‚   â”œâ”€â”€ attention_models.py
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ train_lstm.py
â”œâ”€â”€ train_attention.py
â”œâ”€â”€ train_transformer.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
ğŸ” Results & Observations
Attention-based models outperform vanilla LSTM in capturing long-range dependencies.

Transformers are faster to train and more accurate in headline generation, especially with long input sequences.

Regularization and proper tokenization significantly improve generalization.

ğŸ¤ Credits
Dataset by Sahide Seker

Inspired by concepts from:

Bahdanau et al. (2015)

Luong et al. (2015)

Vaswani et al. (2017): â€œAttention is All You Needâ€

ğŸ“œ License
This project is open-source and available under the MIT License.
